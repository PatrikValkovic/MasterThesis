%
%  An example of a bibliographical database for bibTeX
%
%  Recommended software for maintenance of *.bib files:
%    JabRef, http://jabref.sourceforge.net/
%
%  BEWARE:
%
%    *  If a name contains a capital letter, which must be kept such,
%       use curly brackets ({T}hailand, {HIV}).
%
%  ===========================================================================

@article{cantu1998survey,
  title={A survey of parallel genetic algorithms},
  author={Cant{\'u}-Paz, Erick},
  journal={Calculateurs paralleles, reseaux et systems repartis},
  volume={10},
  number={2},
  pages={141--171},
  year={1998},
  publisher={Citeseer}
}

@book{michalewicz2013solve,
  title={How to solve it: modern heuristics},
  author={Michalewicz, Zbigniew and Fogel, David B},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{cheng2019accelerating,
  title={Accelerating genetic algorithms with {GPU} computing: A selective overview},
  author={Cheng, John Runwei and Gen, Mitsuo},
  journal={Computers \& Industrial Engineering},
  volume={128},
  pages={514--525},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{veronese2010differential,
  title={Differential evolution algorithm on the {GPU} with {C-CUDA}},
  author={Veronese, Lucas de P and Krohling, Renato A},
  booktitle={IEEE Congress on Evolutionary Computation},
  pages={1--7},
  year={2010},
  organization={IEEE}
}

@article{circuitdesignoptimizationea,
author = {Barari, Mansour and Karimi, Hamid Reza and Razaghian, Farhad},
year = {2014},
month = {04},
pages = {1-12},
title = {Analog Circuit Design Optimization Based on Evolutionary Algorithms},
volume = {2014},
journal = {Mathematical Problems in Engineering},
doi = {10.1155/2014/593684}
}

@article{CHENG2019514,
title = {Accelerating genetic algorithms with {GPU} computing: A selective overview},
journal = {Computers \& Industrial Engineering},
volume = {128},
pages = {514-525},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.12.067},
url = {https://www.sciencedirect.com/science/article/pii/S036083521830665X},
author = {John Runwei Cheng and Mitsuo Gen},
keywords = {Parallel genetic algorithms, GPU computing, Parallelism},
abstract = {The emergence of GPU-CPU heterogeneous architectures has led to a fundamental paradigm shift in parallel programming. Accelerating Genetic Algorithms (GAs) on these architectures has received significant attention from both practitioners and researchers ever since GPUs emerged. In the past decade we have witnessed many progresses on migrating parallel GAs from CPU to GPU (Graphical Processing Unit) architecture, which makes this research field truly enter into the world of High Performance Computing (HPC), and demonstrates a great potential to many research disciplines and industrial worlds that can benefit from the power of GPU accelerated stochastic global search to explore large and complex search spaces for better solutions. Designing a parallel algorithm on GPU is quite different from designing one on CPU. On CPU architecture, we typically consider how to distribute data across tens of CPU threads, while on GPU architecture, we have more than hundreds of thousands of GPU threads running simultaneously. Therefore, we should rethink the design approaches and implementation strategies of parallel algorithms to fully utilize the computing power of GPUs to accelerate the computation of GAs. The intention of this paper is to give an overview on selective works of parallel GAs designed for GPU architecture. In this survey paper, we first reexamine the concept of granularity of parallelism for GAs on GPU architecture, discuss how the aspect of data layout affect the kernel design to maximize memory bandwidth, and explain how to organize threads in grid and blocks to expose sufficient parallelism to GPU. The comprehensive overview on selective works since 2010 then follows. The focus is mainly on the perspective of GPU architecture: how to accelerate GAs with GPU computing. Performance issues are not touched in this review, because most of these works are conducted on very early GPU cards, which are out of date already. We finally discuss some future research suggestions in the last section, especially about how to build up an efficient implementation of parallel GAs for hyper-scale computing. Many industrial and academic disciplines will be benefited from the GPU accelerated parallel GAs, one of the promising area is to evolve better deep neural networks.}
}