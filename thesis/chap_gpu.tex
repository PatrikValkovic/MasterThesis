\chapter{GPU Programming}

Increase of processors performance has been for a long time driven by Moore's law -- \enquote{The complexity for minimum component costs has increased at a rate of roughly a factor of two per year. Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain constant for at least 10 years} \citep{MooresLaw}. This \enquote{law}, if we may call it that way, is unfortunately limited by physical properties of components used within modern processors. These limits are both on the side of possible processing speed, and on the side of size and number of transistors in the processor core. Over the last years, scientists are predicting end of Moore's law \citep{MooresLawEnd}, and their voices are louder, since the size of transistors in the \acrfull{acc:cpu} reached $7nm$ in 2018 \citep{SamsungSevenNm}. 

Processors manufactures are well aware of the physical complications arising from the small size of the transistors, and invest more effort into multi--core processors. These processors have a number of independent cores, that can be utilized in parallel and can possibly increase the performance of the processor linearly in respect to the number of cores. To name a few, last server processors by Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} Platinum 8380 Processor with 40 cores and 80 threads \citep{IntelXeonPlatinum}, as well as AMD EPYC\texttrademark\ 7763 with 64 cores and 128 threads \citep{AMDEpyc}, are great example.

\acrfullpl{acc:gpu} takes this idea of multi--cores processors further. Rather than increasing single--thread performance, the \acrshort{acc:gpu} manufactures are focusing on massive parallelism using a great number of cores working in highly parallel and distributed manner \citep{GPUComputingOwens}. Using this approach, the \acrshort{acc:gpu} programming offer promising performance. Common consumer \acrshortpl{acc:gpu} can easily outperform current high--end \acrshortpl{acc:cpu} in the instruction throughput and memory bandwidth. These demands originally came from the game industry, which place great emphasis on parallel processing and reasonably real--time latency. 

Decision about the architecture however influence the way \acrshort{acc:gpu} are programmed. Unlike \acrshort{acc:cpu}, which is can run generic code, \acrshort{acc:gpu} is suited for specific kind of algorithms, that can fully use the underlying hardware. The program needs to have following mandatory properties to make it possible to run on \acrshort{acc:gpu} \citep{GPUComputingOwens}:
\begin{itemize}
    \item \textit{Large computation demands} -- \acrshort{acc:gpu} can deliver enormous performance, but has weak performance for short tasks without a lot of data. The overhead of allocating memory on \acrshort{acc:gpu} and moving it between can easily outweigh the advantage of using it. On the other hand, the real--time rendering require hundreds of operations per pixel, and \acrshort{acc:gpu} can easily meet these demands.
    \item \textit{Significant parallelism} -- the algorithm needs to be easy to parallelize and ideally scale to as many cores as possible. Whereas \acrshort{acc:cpu} provides tens of cores (up to 64 today), the \acrshort{acc:gpu} have hundreds or thousands of them (current cutting-edge \acrshort{acc:gpu} designed especially for \acrshort{acc:ai}, NVIDIA V100, has 5120 CUDA cores \citep{nvidiav100spec}), although with smaller per--core performance. If can algorithm utilize only a few of them, it may be slower than on multi--core \acrshort{acc:cpu}. Once again, real--time rendering can render each pixel independently, which makes it perfect candidate for \acrshort{acc:gpu} computing.
    \item \textit{Throughput over latency} -- because the \acrshort{acc:gpu} is highly parallel, the throughput is more important than the absolute latency of individual operations. From the point of rendering, human eye can perceive pictures in order of milliseconds. Operations in modern processors take in order of nanoseconds. Therefore, the absolute time to render a single pixel is not important as long as all the pixels are rendered in the same order of time. The algorithms cannot make assumptions about the running time of individual parts, but rather on the running time of the task as a whole.
\end{itemize}

\acrlong{acc:ea} fits well into these categories, as the individuals' fitness evaluation may be processed in parallel and independently. The crossover and mutation operators may be parallelized as well, as they operate and individuals or small set of individuals. We may parallelize the algorithm even more by processing each gene separately, as presented by \citet{CHENG2019514}.


%%%%%%%%%%%%%%%
%%           %%
%%  HISTORY  %%
%%           %%
%%%%%%%%%%%%%%%
\section{History}

Historically, \acrshort{acc:gpu} come from needs of game industry for real--time rendering. In the rendering process, a list of geometric primitives, usually triangle, are processed through number of stages to form a final picture. The primitives can be processed independently and therefore in parallel, which makes real--time rendering the ideal case for use of \acrshort{acc:gpu}. The typical operations in graphics consists of \citep{GPUComputingOwens}\index{rendering pipeline}:
\begin{itemize}
    \item Vertex stage, that transforms and compute properties per vertex. Typical operations are transformation of the geometric primitives from world space (that is the coordinates in the scene) into screen space (that is the coordinates in respect to the viewer point of view) and computing additional vertex properties like texture coordinates.
    \item Primitive assembly, that transform geometric primitives into triangles -- the standard geometric primitive for \acrshort{acc:gpu} to understand. This stage may also clip the primitives, that are not in the view of the observer and save some computation in the following stages.
    \item Rasterization takes generated triangles and determine, which pixels correspond to provided triangle. These pixels do not necessary need match number of pixels of the screen and are usually referred as fragments (for example superscaling technique renders picture in higher resolution and then downsample the final picture to match screen resolution \citep{GameGraphicProgramming}). Each triangle may be composed by hundreds or thousands of fragments. Moreover, the rasterization typically interpolate vertex properties between the fragments.
    \item Fragment stage process each fragment and typically determine the final color of the fragment. It may compute fragment interactions with the lights in the scene, fetch colors from textures and so on. This stage is usually most demanding, as typical scene consists of millions of fragments.
    \item Composition stage assembles all the fragments into a final image. It may among other test fragments visibility and their blending.
\end{itemize}

The stages \acrshort{acc:gpu} performs is usually referred as \emph{rendering pipeline} and may slightly differ. At the beginning, stages mentioned above have been preprogrammed by the manufacture and developers have only a limited options how to influence the stages of the pipeline (also known as the fixed--function pipeline). As the software complexity increased, the need for more control arise and the manufactures provided way to program the individual stages of the pipeline. These programs are called \emph{shaders} and their capabilities improved over the years. In 2006, Microsoft introduce Shading Model 4.0, which allowed using same programming interface and hardware both for vertex and fragment shader \citep{DirectX10}. Not only that, but more stages emerged (like tessellation and geometry shaders) to allow developers more control. Today, two major \acrfull{acc:api} exists -- proprietary  DirectX and open--source OpenGL. Note that some operations of the rendering pipeline, such as primitive assembly, rasterization, and composition are still implemented by the \acrshort{acc:gpu} manufactures and in most cases are implemented by special--purpose hardware components \citep{SoftwareRasterization}. They are therefore integral part of the rendering pipeline and are not possible to modify.

Soon, scientists and engineers identified many more problems suitable for processing on \acrshort{acc:gpu}. Although some parts of the rendering pipeline were fully programmable, the pipeline still inherently graphical and some stages are not possible to skip. To run general--purpose programs on \acrshort{acc:gpu} required encoding the problem in the domain of geometric primitives and use available programmable and special--purpose hardware to implement the algorithm. Moreover, the programmers need to decide which part of the hardware will execute which part of the calculation and how the pipeline's fixed stages will affect the data structure. As shader programs do not have access to shared memory, the data passing were realized using textures, vertices, and fragment properties \citep{GPUComputingOwens}. These complications discouraged general use of \acrshort{acc:gpu} for general--purpose programs, although some efforts have been made to hide underlying rendering pipeline from the programmer point of view \citep{BrookGPU}.

Lack of support for general--purpose program on \acrshort{acc:gpu} inspired companies to develop new approach, that would give programmers better control over the underlying hardware. In 2006, NVIDIA introduced first version of \acrfull{acc:cuda} \citep{CUDAwiki}. Open source implementation of general--purpose programming came in 2008 as Open Computing Language (OpenCL) by Khronos Group \citep{OpenCLRelease}. These are higher level interfaces with C--like syntax exposing \acrshort{acc:gpu} resources without any connection to the graphical pipeline. Programmer is given control of execution and parallelization of the program, as well as memory access. Over the years, these technologies undergo a number of improvements (current version is CUDA 11.2, respectively OpenCL 3.0) adding support for data types, language constructs and hardware.

\acrshort{acc:cuda} become de facto standard in the field of general--purpose computing on \acrshort{acc:gpu} and I will focus primarily on it. Fortunately, other technologies like OpenCL have similar underlying architecture and approach to the \acrshort{acc:gpu} programming. Following section should therefore generalize for them as well.




%%%%%%%%%%%%%%%%%%%%
%%                %%
%%  ARCHITECTURE  %%
%%                %%
%%%%%%%%%%%%%%%%%%%%
\section{Architecture}

Design goals of \acrshort{acc:gpu} is to provide in order of magnitude higher instruction throughput and memory bandwidth in comparison to \acrshort{acc:cpu}. The overall architecture of \acrshort{acc:gpu} is therefore significantly different. In this section, I will focus on \acrshort{acc:gpu} architecture from the point of \acrshort{acc:cuda} platform, taking inspiration mainly from the CUDA Programming Guide \citep{CUDAguide}.

\begin{figure}
    \begin{subfigure}[t]{0.47\textwidth}
        \includegraphics[width=\textwidth]{img/master_cpu_arch.pdf}
        \caption{Architecture of \acrshort*{acc:cpu}. Each core has L1 cache and own control circuit. Cores can therefore work independently.}
        \label{fig:cpuarch}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.47\textwidth}
        \includegraphics[width=\textwidth]{img/master_gpu_arch.pdf}
        \caption{Architecture of \acrshort*{acc:gpu}. One control circuit controls number of cores. One row of cores is called \acrlong{acc:sm} (\acrshort{acc:sm}, bounded by gray borders). L2 cache is shared between the cores.}
        \label{fig:gpuarch}
    \end{subfigure}
    \caption{Difference between \acrshort*{acc:cpu} and \acrshort*{acc:gpu} architecture}
\end{figure}

Computation power of \acrshort{acc:gpu} comes from the fact, that it devotes in order of magnitudes more transistors on the chip to the data processing, rather than to the control circuits. The architecture of the \acrshort{acc:cpu} is shown in figure \ref{fig:cpuarch}. Each core has its own control circuit and therefore each cores can execute instructions independently. Moreover, each thread has its own L1 cache.

The \acrshort{acc:gpu}\index{GPU!architecture} architecture is depicted in figure \ref{fig:gpuarch}. A set of cores share the same control circuit and L1 cache. One row of these cores is called \acrfull{acc:sm}. Number of cores per \acrshort{acc:sm} differ between architectures, more information can be found in the article by \citet{NVIDIAhistory} or on NVIDIA official websites. Using one control circuit to manage multiple cores brings some implications -- all the cores needs to execute the same instruction at once. In this sense, the instruction throughput is achieved using \acrfull{acc:simd} approach -- one instruction can at one tick modify multiple data. This is faster than executing the same instruction sequentially on \acrshort{acc:cpu}.

Each \acrshort{acc:gpu} come with different set of hardware features or available instructions. Compute capability\index{GPU!compute capability} specify, which features are available at the target hardware. For example half--precision float--point operations are available from compute capability 5.3. Number of arithmetic CUDA cores per \acrshort{acc:sm} is 8 for compute capability 1.x, 32 resp. 48 for compute capability 2.0 resp. 2.1, 64 for compute capability 6.0, 7.x, and 8.0, 128 for compute capability 6.1, 6.2, and 8.6, and finally 192 for compute capability 3.x. I will therefore not discuss specific numbers in the following text. In order to use newer compute capabilities, the \acrshort{acc:gpu}, driver and the software need to support them.

\acrlong{acc:cuda} allows almost direct translation of code from C onto the \acrshort{acc:gpu}. It uses C++ language with a few extensions. It is therefore straightforward to write program for \acrshort{acc:gpu} as long as we are familiar with C syntax. To distinguish code running on \acrshort{acc:cpu} and \acrshort{acc:gpu}, \acrshort{acc:cuda} differentiate between host (the code running on \acrshort{acc:cpu}) and device (the code running on the \acrshort{acc:gpu}) functions. There is special type of functions, called \emph{kernels}\index{CUDA!kernel}, that is called by the host, but executed on the device.

The \acrshort{acc:cuda} defines following terminology:
\begin{itemize}
    \item CUDA thread\index{CUDA!thread} is execution of single kernel. To process $K$ blocks of data, the runtime creates at least $K$ threads and run the same kernel function on each of them.
    \item CUDA thread block\index{CUDA!block} is a set of threads grouped into grid. Programmer may specify size of the grid during call of the kernel function as a three--dimensional vector. The number of threads in the thread block is limited by the compute capability of the hardware. Moreover, all the threads in the thread block needs to be executed on one \acrshort{acc:sm} (with some exceptions) and can be synchronized using \textit{\_\_syncthreads} call. Each kernel receives \textit{threadIdx} variable, which allows to identify individuals threads in the block.
    \item CUDA kernel grid\index{CUDA!grid} is grid of thread blocks. Similarly to thread block, its size may be specified during  kernel call and each thread receives \textit{blockIdx} resp. \textit{blockDim} variables defining block in which the thread is running, resp. size of the blocks. More blocks from the same grid may run in parallel on different or same \acrshort{acc:sm}, but the threads cannot be synchronized between the blocks.
    \item Warp\index{CUDA!warp} is set of 32 threads (for all compute capabilities so far), that execute the same instruction. The thread block is split into warps and executed by the \acrshort{acc:sm}. The warp is the minimal unit executable on the \acrshort{acc:gpu}. The device plan warps the same way \acrshort{acc:os} plans processes and may switch context to other warp while the current one is waiting for the data. By executing different warp, the latency of for example memory access is \enquote{hidden} behind execution of different warp. Although \acrshort{acc:gpu} cannot execute anything smaller than warp, it may mask some threads from the execution. It is therefore possible to have block size of $50$ threads, however $2\cdot32 - 50=14$ cores in the second warp will still execute (although this fact is hidden from the programmer).
\end{itemize}


%TODO moving memory
%TODO memory hierarchy and shared memory banks
%TODO trans warp functions



%%%%%%%%%%%%%%%
%%           %%
%%  PYTORCH  %%
%%           %%
%%%%%%%%%%%%%%%
\section{PyTorch}




%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                       %%
%%  EVA PARALLELIZATION  %%
%%                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evolutionary Algorithms parallelization}
