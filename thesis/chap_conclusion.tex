\chapter{Conclusion}
\label{chap:conclusion}

This work summarizes current knowledge of \acrlong*{acc:ea} and gives a comprehensive overview of this research field. It discusses the well--known evolutionary operators and gives their detailed analysis from the implementation point of view, along with their desired properties. The work further describes the three most common evolutionary algorithms -- the \acrlong{acc:ga}, real--coded evolutionary algorithms, and the \acrlong{acc:pso} algorithms. Each of them is analyzed, and their representative operators are described and examined.

The work further discusses the differences between \cpu and \gpu and their architectures. I show that while the \gpu has its application in various fields mentioned in the work, the mindset for programming on \gpu is considerably different and unique. I present the OpenCL and \cuda technologies, and present the terminology used in \gpu programming. The work shows the elements of \cuda programming, with a particular focus on a performance issue that can arise. 

I then present the PyTorch library for the Python programming language, describe its architecture and functionality. The work put into connection the PyTorch library and \cuda programming discussed before. Follows discussion about possible ways to parallelize \acrlong{acc:ea} both on \cpu and \gpuns. The work focuses on different individual encoding, parallelization granularity, and evaluation architectures.

Follows presentation of the \acrfull{acc:ffeat}, my contribution to this area. I discuss in detail its functionality, implementation decisions, architecture, and the proposed workflow. The work gives extra space to challenging parts of the implementation, like parental sampling and crossover schemes. Some of these problems arise from the limitations of the PyTorch library and the \cuda programming in general; these aspects are discussed. Implementation of some operators is then shown in the text, and I discuss possible ways to extend the library. Overall, I show that the proposed implementation is easy to extend, operators are written in a readable manner and runs efficiently on \gpuns.

The implementation is subjected to extensive testing on \acrshort{acc:3sat} problem and \acrshort{acc:bbob} test suite. The experiments show the advantage of using \cuda implementation for medium and big--sized populations and problems. The experiments show an order of magnitude speedup using \cuda implementation rather than \cpuns. Moreover, the experiments show the advantage of a bigger population on some problems while leading to premature convergence on others. I discuss this phenomenon and explain the behavior of algorithms.

Finally, I compare the proposed implementation against the native \cpp implementation on the \acrshort{acc:3sat} problem. It shows a noticeable slowdown on one core machine but gets faster as the number of cores increases. The proposed implementation makes use of the parallelized operators implementation and becomes dominant. The comparison with \cpp implementation confirms the superiority of \cuda implementation for medium and big--sized populations.

This work dealt with \acrshort{acc:ga}, real--coded evolutionary algorithms, and \acrshort{acc:pso} algorithms. There are other classes of algorithms that the implementation was not tested on. Further work could expand the library with these algorithms. I believe the \acrshort{acc:cma} would greatly benefit of \cuda implementation. Benchmark the implementation on permutation--based algorithms may seems interesting as well. Furthermore, all the presented problems have been vectorized, and the \gpu implementation could evaluate the whole population at once. It may be interesting to explore the possibility of evaluating each individual separately, taking away the major advantage of \gpu implementation. This may be practical for complicated fitness functions. Future work could explore this possibility and evaluate whether using \gpu implementation keeps its advantages.